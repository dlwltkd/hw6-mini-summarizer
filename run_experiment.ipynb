{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a20873",
   "metadata": {},
   "source": [
    "##Basic setup & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa55e07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\birke\\OneDrive\\Desktop\\projects\\hw6_mini_summarizer\\hw6-mini-summarizer\n",
      "Src path: c:\\Users\\birke\\OneDrive\\Desktop\\projects\\hw6_mini_summarizer\\hw6-mini-summarizer\\src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "project_root = os.getcwd()\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Src path:\",  src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea13f14",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Now import  modules\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataset_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_data, train_test_split\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_baseline_on_dataset, compute_rouge_scores, average_rouge\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel_pipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_summarizer, run_model_on_dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\birke\\OneDrive\\Desktop\\projects\\hw6_mini_summarizer\\hw6-mini-summarizer\\src\\evaluate.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbaseline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lead1_summary\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrouge_score\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rouge_scorer\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_baseline_on_dataset\u001b[39m(examples: List[Dict]) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m      8\u001b[39m     predictions = []\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'rouge_score'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Now import  modules\n",
    "from dataset_utils import load_data, train_test_split\n",
    "from evaluate import run_baseline_on_dataset, compute_rouge_scores, average_rouge\n",
    "from model_pipeline import load_summarizer, run_model_on_dataset\n",
    "from build_dataset import build_dataset, clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e2b2b",
   "metadata": {},
   "source": [
    "##build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0368463",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ae50c",
   "metadata": {},
   "source": [
    "##Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9829067",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/summarization_data.json\"\n",
    "examples = load_data(data_path)\n",
    "\n",
    "len(examples), examples[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f9c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = [ex for ex in examples if ex.get(\"split\") == \"train\"]\n",
    "test_examples  = [ex for ex in examples if ex.get(\"split\") == \"test\"]\n",
    "\n",
    "len(train_examples), len(test_examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350510b",
   "metadata": {},
   "source": [
    "##Prepare gold summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6647493",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_summaries = [ex[\"summary\"] for ex in test_examples]\n",
    "len(gold_summaries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d62f1",
   "metadata": {},
   "source": [
    "##Run baseline on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63951cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_preds = run_baseline_on_dataset(test_examples)\n",
    "\n",
    "len(baseline_preds), baseline_preds[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e545d",
   "metadata": {},
   "source": [
    "##Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = load_summarizer(device=0)  # 0 = GPU, -1 = CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed4e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick test\n",
    "test_text = test_examples[0][\"document\"][:1000]  # truncate for speed\n",
    "summary_example = summarizer(test_text, max_new_tokens=60, min_length=15, truncation=True)[0][\"summary_text\"]\n",
    "summary_example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4533018",
   "metadata": {},
   "source": [
    "##Run Model on full test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f03b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = run_model_on_dataset(\n",
    "    summarizer,\n",
    "    test_examples,\n",
    "    min_len=15,\n",
    "    max_new_tokens=60\n",
    ")\n",
    "\n",
    "len(model_preds), model_preds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d56271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see example\n",
    "i = 0\n",
    "print(\"GOLD SUMMARY:\\n\", gold_summaries[i], \"\\n\")\n",
    "print(\"MODEL SUMMARY:\\n\", model_preds[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239360be",
   "metadata": {},
   "source": [
    "##Compute ROUGE for baseline and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2019fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline ROUGE\n",
    "baseline_scores = compute_rouge_scores(gold_summaries, baseline_preds)\n",
    "baseline_avg = average_rouge(baseline_scores)\n",
    "\n",
    "\n",
    "# Model ROUGE\n",
    "model_scores = compute_rouge_scores(gold_summaries, model_preds)\n",
    "model_avg = average_rouge(model_scores)\n",
    "\n",
    "\n",
    "def print_rouge(name, avg):\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  ROUGE-1 F: {avg['rouge1']:.4f}\")\n",
    "    print(f\"  ROUGE-L F: {avg['rougeL']:.4f}\")\n",
    "\n",
    "print_rouge(\"Baseline\", baseline_avg)\n",
    "print_rouge(\"Model\", model_avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e31c823",
   "metadata": {},
   "source": [
    "#View graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a06c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_r1 = [s[\"rouge1\"].fmeasure for s in baseline_scores]\n",
    "model_r1    = [s[\"rouge1\"].fmeasure for s in model_scores]\n",
    "\n",
    "len(baseline_r1), len(model_r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "improvements = np.array(model_r1) - np.array(baseline_r1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(improvements, bins=15)\n",
    "\n",
    "plt.xlabel(\"ROUGE-1 Improvement (Model - Baseline)\")\n",
    "plt.ylabel(\"Number of Examples\")\n",
    "plt.title(\"Distribution of ROUGE-1 Improvement\")\n",
    "\n",
    "plt.axvline(0.0, linestyle=\"--\")  # zero line\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean improvement:\", improvements.mean())\n",
    "print(\"Min improvement:\", improvements.min())\n",
    "print(\"Max improvement:\", improvements.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_length(text: str) -> int:\n",
    "    return len(text.split())  # word count\n",
    "\n",
    "doc_lengths = [text_length(ex[\"document\"]) for ex in test_examples]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(doc_lengths, model_r1, alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Document Length (words)\")\n",
    "plt.ylabel(\"ROUGE-1 F-measure (Model)\")\n",
    "plt.title(\"Model ROUGE-1 vs Document Length\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9d735",
   "metadata": {},
   "source": [
    "#save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac795780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output = {\n",
    "    \"gold\": gold_summaries,\n",
    "    \"baseline\": baseline_preds,\n",
    "    \"model\": model_preds,\n",
    "}\n",
    "\n",
    "with open(\"data/predictions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved predictions to data/predictions.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3342dc5",
   "metadata": {},
   "source": [
    "##Pick random Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb65b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "indices = random.sample(range(len(test_examples)), 3)\n",
    "indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c280ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in indices:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Example {i}\")\n",
    "    print(\"\\nDOCUMENT:\\n\", test_examples[i][\"document\"][:800], \"...\\n\")\n",
    "    print(\"GOLD SUMMARY:\\n\", gold_summaries[i], \"\\n\")\n",
    "    print(\"BASELINE SUMMARY:\\n\", baseline_preds[i], \"\\n\")\n",
    "    print(\"MODEL SUMMARY:\\n\", model_preds[i], \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
